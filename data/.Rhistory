# summaries of the scaled variables
summary(boston_std)
# class of the boston_scaled object
class(boston_std)
# change the object to data frame
boston_scaled <- as.data.frame(boston_std)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))
# look at the table of the new factor crime
table(crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# number of rows in the Boston dataset
n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set
test <- boston_scaled[-ind,]
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col="orange", length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col= "orange", pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
boston_std <- as.data.frame(boston_std)
boston_dist <- dist(boston_std)
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_std, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# K-means clustering
km <-kmeans(boston_std, centers = 2)
# plot the Boston dataset with clusters
#pairs(Boston, col = km$cluster)
class(km$cluster)
model_predictors <- dplyr::select(train, -crime)
dim(model_predictors)
dim(lda.fit$scaling)
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crime)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crime)
n_k <- nrow(Boston)
ind_k <- sample(n_k, size = n_k*0.8)
train_k <- Boston[ind_k,]
train_k <- scale(train_k)
twcss2 <- sapply(1:k_max, function(k){kmeans(stats::na.omit(train_k), k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss2, geom = 'line') # 2
# k-means clustering
km_k <-kmeans(train_k, centers = 2)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km_k$cluster)
km_l <-kmeans(train_k, centers = 4)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km_l$cluster)
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crime)
# Let's find k-clusters for train set
n_k <- nrow(Boston)
ind_k <- sample(n_k, size = n_k*0.8)
train_k <- Boston[ind_k,]
train_k <- scale(train_k)
twcss2 <- sapply(1:k_max, function(k){kmeans(stats::na.omit(train_k), k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss2, geom = 'line') # 2
# k-means clustering
km_k <-kmeans(train_k, centers = 2)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km_k$cluster)
# Let's increase the number of clusters to 4
km_l <-kmeans(train_k, centers = 4)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = km_l$cluster)
library(dplyr)
library(stringr)
##############################################
# Week 4
# Import data
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
# Meta files: http://hdr.undp.org/en/content/human-development-index-hdi
# http://hdr.undp.org/sites/default/files/hdr2015_technical_notes.pdf
# hd
View(hd)
str(hd)
dim(hd)
summary(hd)
# gii
View(gii)
str(gii)
dim(gii)
summary(gii)
# Rename variables
# hd
colnames(hd) <- c("rank_hdi", "country", "hdi", "life_exp", "exp_years_education", "mean_years_education", "gni", "gni_min_hdi")
# gii
colnames(gii) <- c("rank_gii", "country", "gii", "maternal_mortality_ratio", "adolescent_birth_rate", "female_parliament", "sec_edu_female", "sec_edu_male", "labour_rate_female", "labour_rate_male")
# New variables
# Ratio of Female and Male populations with secondary education in each country
gii <- mutate(gii, sec_edu_ratio = sec_edu_female/sec_edu_male)
# Ratio of labour force participation of females and males in each country
gii <- mutate(gii, labour_rate_ratio = labour_rate_female/labour_rate_male)
# Combine two datasets
human <- inner_join(hd, gii, by = "country")
View(human)
# Write data in csv
#setwd("C:/Users/juhol/OneDrive/Documents/GitHub/IODS-project/data")
#write.csv(human, file = "human.csv")
############################################################
# Week 5
str(human)
human$gni <- str_replace(human$gni, pattern=",", replace ="") %>% as.numeric()
colnames(human)
human <- select(human,- mean_years_education, - gni_min_hdi, - rank_gii, - gii, - sec_edu_female, - sec_edu_male, - labour_rate_female, - labour_rate_male, -mean_years_education, - gni_min_hdi, - rank_gii, - gii, - sec_edu_female, - sec_edu_male, - labour_rate_female, - labour_rate_male, - rank_hdi, - country, - gii)
complete.cases(human)
data.frame(human[-1], comp = complete.cases(human))
# filter out all rows with NA values
human <- filter(human, complete.cases(human) == T)
colnames(human)
View(human)
# Juho Lahteenmaa
# 24/11/2018
# IODS week 4
# Data wrangling
##############################################
# Packages
library(dplyr)
library(stringr)
##############################################
# Week 4
# Import data
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
# Meta files: http://hdr.undp.org/en/content/human-development-index-hdi
# http://hdr.undp.org/sites/default/files/hdr2015_technical_notes.pdf
# hd
View(hd)
str(hd)
dim(hd)
summary(hd)
# gii
View(gii)
str(gii)
dim(gii)
summary(gii)
# Rename variables
# hd
colnames(hd) <- c("rank_hdi", "country", "hdi", "life_exp", "exp_years_education", "mean_years_education", "gni", "gni_min_hdi")
# gii
colnames(gii) <- c("rank_gii", "country", "gii", "maternal_mortality_ratio", "adolescent_birth_rate", "female_parliament", "sec_edu_female", "sec_edu_male", "labour_rate_female", "labour_rate_male")
# New variables
# Ratio of Female and Male populations with secondary education in each country
gii <- mutate(gii, sec_edu_ratio = sec_edu_female/sec_edu_male)
# Ratio of labour force participation of females and males in each country
gii <- mutate(gii, labour_rate_ratio = labour_rate_female/labour_rate_male)
# Combine two datasets
human <- inner_join(hd, gii, by = "country")
View(human)
# Write data in csv
#setwd("C:/Users/juhol/OneDrive/Documents/GitHub/IODS-project/data")
#write.csv(human, file = "human.csv")
############################################################
str(human)
human$gni <- str_replace(human$gni, pattern=",", replace ="") %>% as.numeric()
colnames(human)
# RM unneeded columns
human <- select(human, - rank_hdi,  - hdi, - mean_years_education,  - gni_min_hdi, -rank_gii,  - gii, -sec_edu_female,  - sec_edu_male, -labour_rate_female, - labour_rate_male)
colnames(human)
# filter out all rows with NA values
human <- filter(human, complete.cases(human) == T)
colnames(human)
human$country
# define the last indice we want to keep
last <- nrow(human) - 7
# choose everything until the last 7 observations
human <- human[1:last, ]
# add countries as rownames
rownames(human) <- human$country
# define the last indice we want to keep
last <- nrow(human) - 7
human <- select(human, - country)
setwd("C:/Users/juhol/OneDrive/Documents/GitHub/IODS-project/data")
?write.csv
write.csv(human, file = "human.csv", row.names = T)
?read.csv
human <- read.csv("C:/Users/juhol/OneDrive/Documents/GitHub/IODS-project/data", sep  =",", header = T)
human <- read.csv("C:/Users/juhol/OneDrive/Documents/GitHub/IODS-project/data/human.csv", sep  =",", header = T)
# look at the (column) names of human
names(human)
?read_csv
# look at the structure of human
str(human)
View(human)
human <- read_csv("C:/Users/juhol/OneDrive/Documents/GitHub/IODS-project/data/human.csv", sep  =",", header = T, row.names = 1)
human <- read.csv("C:/Users/juhol/OneDrive/Documents/GitHub/IODS-project/data/human.csv", sep  =",", header = T, row.names = 1)
# look at the (column) names of human
names(human)
# look at the structure of human
str(human)
# print out summaries of the variables
summary(human)
View(human)
library(GGally)
library(ggplot2)
library(dplyr)
# visualize the 'human' variables
ggpairs(human)
# compute the correlation matrix and visualize it with corrplot
cor(human) %>% corrplot
library(corrplot)
# compute the correlation matrix and visualize it with corrplot
cor(human) %>% corrplot
pca_human_not_std <- prcomp(human)
sum_pca_human_not_std <- summary(pca_human_not_std)
# rounded percetanges of variance captured by each PC
pca_pr_not_std <- round(100*s$importance[2, ], digits = 1)
# rounded percetanges of variance captured by each PC
pca_pr_not_std <- round(100*sum_pca_human_not_std$importance[2, ], digits = 1)
# print out the percentages of variance
pca_pr_not_std
pca_human_not_std <- prcomp(human)
pca_human_not_std
sum_pca_human_not_std <- summary(pca_human_not_std)
sum_pca_human_not_std
sum_pca_human_not_std$importance[2, ]
# rounded percetanges of variance captured by each PC
pca_pr_not_std <- round(100*sum_pca_human_not_std$importance[2, ], digits = 3)
# print out the percentages of variance
pca_pr_not_std
# create object pc_lab to be used as axis labels
pc_lab_not_std <- paste0(names(pca_pr_not_std), " (", pca_pr_not_std, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab_not_std[1], ylab = pc_lab_not_std[2])
# draw a biplot
biplot(pca_human_not_std, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab_not_std[1], ylab = pc_lab_not_std[2])
human_std <- scale(human)
warnings()
human_std <- scale(human)
sum_pca_human <- summary(pca_human)
pca_human <- prcomp(human_std)
sum_pca_human <- summary(pca_human)
# rounded percetanges of variance captured by each PC
pca_pr <- round(100*sum_pca_human$importance[2, ], digits = 3)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
# print out the percentages of variance
pca_pr_not_std
qplot(pca_pr_not_std)
# print out the percentages of variance
pca_pr_not_std
qplot( y = pca_pr_not_std)
ggplot(pca_pr_not_std, aes(y = pca_pr_not_std)) + geom_histogram()
sum_pca_human_not_std
ggplot(pca_pr_not_std, aes(round(100*sum_pca_human_not_std$importance[2, ], digits = 3))) + geom_histogram()
ggplot(sum_pca_human_not_std, aes(round(100*importance[2, ], digits = 3))) + geom_histogram()
qplot(pca_pr_not_std)
qplot(y = pca_pr_not_std)
qplot(y = pca_pr_not_std) + geom_histogram()
qplot(pca_pr_not_std) + geom_histogram()
# print out the percentages of variance
pca_pr_not_std
qplot(data = pca_pr_not_std, x = pca_human_not_std$importance[1,], y = pca_human_not_std$importance[2,] ) + geom_histogram()
# histogram
df.pca_pr_not_std <- as.data.frame(pca_pr_not_std)
qplot(data = df.pca_pr_not_std, y = df.pca_pr_not_std$pca_pr_not_std) + geom_histogram()
qplot(pca_pr_not_std) + geom_histogram()
str(df.pca_pr_not_std)
View(df.pca_pr_not_std)
row.names(df.pca_pr_not_std)
qplot(pca_pr_not_std, x = row.names(df.pca_pr_not_std)) + geom_histogram()
ggplot(data = df.pca_pr_not_std, aes(row.names(df.pca_pr_not_std), df.pca_pr_not_std))
ggplot(data = df.pca_pr_not_std, aes(row.names(df.pca_pr_not_std), df.pca_pr_not_std$pca_pr_not_std)) + geom_histogram()
qplot(pca_human_not_std)
pca_human_not_std
qplot(y = pca_human_not_std$sdev )
qplot(y = pca_human_not_std$sdev ) + geom_histogram()
qplot(y = pca_human_not_std$sdev ) + geom_histogram((binwidth = 5)
qplot(y = pca_human_not_std$sdev ) + geom_histogram((binwidth = 5)
qplot(y = pca_human_not_std$sdev ) + geom_histogram(binwidth = 5)
# draw a biplot
biplot(pca_human_not_std, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab_not_std[1], ylab = pc_lab_not_std[2])
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
warnings()
# draw a biplot
?par()
plot(table(pca_pr_not_std))
table(pca_pr_not_std)
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
library(plotly)
pca_human_not_std
install.packages("FactoMineR")
tea <- data(tea)
tea <- read.table("http://factominer.free.fr/book/tea.csv",header=TRUE,sep=";")
colnames(tea)
str(tea)
summary(tea)
ggpairs(tea)
summary(tea)
# visualize the dataset
gather(tea) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
library(tidyr)
# visualize the dataset
gather(tea) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
tea
gather(pca_pr_not_std) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_histogram()
class(tea)
class(pca_pr_not_std)
pca_pr_not_std <- as.data.frame(pca_pr_not_std)
gather(pca_pr_not_std) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_histogram()
# visualize the dataset
gather(tea) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
# visualize the dataset
gather(tea) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
# visualize the dataset
gather(tea) %>% ggplot(aes(value))
ther(tea) %>% ggplot(aes(value))
+ facet_wrap("key", scales = "free_x")
+ geom_bar()
+ theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
# visualize the dataset
gather(tea) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free_x") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
# visualize the dataset
gather(tea) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free_y") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
# visualize the dataset
gather(tea) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
length(tea)
length(tea)/2
tea[,1:length(tea)/2]
colnames(tea[,1:length(tea)/2])
tea[,1]
colnames(tea[,1])
colnames(tea[,2])
View(tea[,2])
View(tea[,3])
View(tea)
# Splitting the plot
colnames(tea)
tea_subset_1 <- dplyr::select(tea, breakfast:pub)
tea_subset_2 <- dplyr::select(tea, variety:frequency)
tea_subset_3 <- dplyr::select(tea, exotic:no.effect.health)
# visualize the dataset
gather(tea_subset_1) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
gather(tea_subset_2) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
tea_subset_2 <- dplyr::select(tea, variety:frequency) %>% select(-age)
gather(tea_subset_2) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
gather(tea_subset_3) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
source('~/GitHub/IODS-project/own/week_5_Rmd.R', echo=TRUE)
?tea
library(FactoMineR)
# multiple correspondence analysis
mca <- MCA(tea, graph = FALSE)
# summary of the model
summary(mca)
# multiple correspondence analysis
mca <- MCA(tea, graph = FALSE)
# summary of the model
summary(mca)
tea
?which
complete.cases(tea)
which((1:12)%%2 == 0) # which are even?
complete_cases <- complete.cases(tea)
length(which(complete_cases == F))
length(which(complete_cases == T))
gather(tea_subset_1) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
gather(tea_subset_2) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
gather(tea_subset_3) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
colnames(tea)
dplyr::select(tea, variety) %>% dplyr::distinct()
dplyr::select(tea, how) %>% dplyr::distinct()
dplyr::select(tea, format) %>% dplyr::distinct()
dplyr::select(tea, place.of.purchase) %>% dplyr::distinct()
dplyr::select(tea, type) %>% dplyr::distinct()
dplyr::select(tea, frequency) %>% dplyr::distinct()
tea_time <- dplyr::select(tea, one_of(exotic, spirituality, good.for.health, diuretic, friendliness, iron.absorption, feminine, refined, slimming, stimulant, relaxant, no.effect.health))
tea_time <- dplyr::select(tea, exotic, spirituality, good.for.health, diuretic, friendliness, iron.absorption, feminine, refined, slimming, stimulant, relaxant, no.effect.health)
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
# multiple correspondence analysis
mca <- MCA(tea_time, graph = FALSE)
# summary of the model
summary(mca)
# visualize MCA
plot(mca, invisible=c("ind"), habillage = "quali")
# summary of the model
s_tea <- summary(mca)
# summary of the model
s_tea <- summary(mca)
s_tea$importance
s_tea$importance
s_tea
# summary of the model
s_tea <- summary(mca)
s_tea
rm(s_tea)
# summary of the model
summary(mca)
# look at the (column) names of human
names(human)
# look at the (column) names of human
names(human)
# look at the structure of human
str(human)
# visualize the 'human' variables
ggpairs(human)
my_fn <- function(data, mapping, ...){
p <- ggplot(data = data, mapping = mapping) +
geom_point() +
geom_smooth(method=loess, fill="red", color="red", ...) +
geom_smooth(method=lm, fill="blue", color="blue", ...)
p
}
ggpairs(human, lower = list(continuous = my_fn))
# print out summaries of the variables
summary(human)
ggpairs(human, lower = list(continuous = my_fn))
# compute the correlation matrix and visualize it with corrplot
cor(human) %>% corrplot
# compute the correlation matrix and visualize it with corrplot
cor(human)
order(cor(human))
ggpairs(human, lower = list(continuous = my_fn))
220/1.8 #1 km/h = 1000m/1800s
300*1.8
pca_human_not_std <- prcomp(human)
sum_pca_human_not_std <- summary(pca_human_not_std)
# rounded percetanges of variance captured by each PC
pca_pr_not_std <- round(100*sum_pca_human_not_std$importance[2, ], digits = 3)
# print out the percentages of variance
pca_pr_not_std
# create object pc_lab to be used as axis labels
pc_lab_not_std <- paste0(names(pca_pr_not_std), " (", pca_pr_not_std, "%)")
# draw a biplot
biplot(pca_human_not_std, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab_not_std[1], ylab = pc_lab_not_std[2])
sum_pca_human_not_std
human_std <- scale(human)
pca_human <- prcomp(human_std)
sum_pca_human <- summary(pca_human)
sum_pca_human
# rounded percetanges of variance captured by each PC
pca_pr <- round(100*sum_pca_human$importance[2, ], digits = 3)
# print out the percentages of variance
pca_pr
53.605 + 53.605
53.605 + 16.237
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
pca_human
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
cor(human) %>% corrplot
ggpairs(human, lower = list(continuous = my_fn))
str(tea)
summary(tea)
# Splitting the plot
colnames(tea)
tea_subset_1 <- dplyr::select(tea, breakfast:pub)
tea_subset_2 <- dplyr::select(tea, variety:frequency) %>% select(-age)
tea_subset_3 <- dplyr::select(tea, exotic:no.effect.health)
# visualize the dataset
gather(tea_subset_1) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
gather(tea_subset_2) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
gather(tea_subset_3) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
tea_time <- dplyr::select(tea, exotic, spirituality, good.for.health, diuretic, friendliness, iron.absorption, feminine, refined, slimming, stimulant, relaxant, no.effect.health)
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
# multiple correspondence analysis
mca <- MCA(tea_time, graph = FALSE)
# summary of the model
summary(mca)
# visualize MCA
plot(mca, invisible=c("ind"), habillage = "quali")
14.14+12.07
warnings()
