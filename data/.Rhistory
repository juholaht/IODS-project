#korjattuSSE
#korjauskerroin = sqrt(korjattuSSE)/sqrt(korjaamatonSSE)
#korjauskerroin
# Kertoimet ovat oikeat
#########################################################################################
#summaryR.lm <- function(model, type=c("hc3", "hc0", "hc1", "hc2", "hc4"), ...){
#
#  if (!require(car)) stop("Required car package is missing.")
#
#  type <- match.arg(type)
#  V <- hccm(model, type=type)
#  sumry <- summary(model)
#  table <- coef(sumry)
#  table[,2] <- sqrt(diag(V))
#  table[,3] <- table[,1]/table[,2]
#  table[,4] <- 2*pt(abs(table[,3]), df.residual(model), lower.tail=FALSE)
#
#  sumry$coefficients <- table
#  p <- nrow(table)
#  hyp <- cbind(0, diag(p - 1))
#  sumry$fstatistic[1] <- linearHypothesis(model, hyp,white.adjust=type)[2,"F"]
#
#  print(sumry)
#  return(sumry)
# cat("Note: Heteroscedasticity-consistent standard errors using adjustment", type, "\n")
#
#}
#################################################################################################
# Lasketaan mallin pitkan- ja lyhyen aikavalin tulojoustot
# Lyhyt
beta_bktTuotot <- 0.025892
kaBkt <- sum(bktPerCapita_t)/length(bktPerCapita_t)
kaTuotot <- sum(kateKotimaa_t)/length(kateKotimaa_t)
tulojoustoSt <- beta_bktTuotot*(kaBkt/kaTuotot)
# Pitka
beta_TuototLagged <- 0.72464
tulojoustoLt <- (beta_bktTuotot/(1-beta_TuototLagged))*(kaBkt/kaTuotottulo)
# 95-porsentin luottamusvali selittavalle muuttujalle
beta <- -2.2902
ser <- 0.60333
luottamusvaliYsiViis <- c(beta - ser*1.96, beta + ser*1.96)
luottamusvaliYsiViis
# Piirretään kuvaaja
plot(vuosi_t ,kateKotimaa_t, ylim = c(0, 2000),type = "o", col = "blue", xlab = "Vuosi", ylab = "Pelituotto = Liikevaihto - Pelaajille maksetut voitot (€)")
lines(vuosi_t, kateUlkomaat_t, type = "o", col  = "red")
plot(vuosi_t ,kateUlkomaat_t, ylim = c(0, 500),type = "o", col = "red", xlab = "Vuosi", ylab = "Pelikate = Liikevaihto - Pelaajille maksetut voitot (€)", main = "Pelikate ulkomaat")
# Piiretään muutoskuvaaja
# Muutokset kotimaan pelikatteille
dKateKotimaa_t <- kateKotimaa_t - kateKotimaa_tMin1
dKateKotimaa_t <- dKateKotimaa_t/kateKotimaa_tMin1
dKateKotimaa_t <- dKateKotimaa_t*100
cbind(dKateKotimaa_t, vuosi_t)
dKateKotimaa2005_2016 <- subset(dKateKotimaa_t, vuosi_t >2004)
mean(dKateKotimaa2005_2016)
# Muutokset ulkomaan pelikatteille
dKateUlkomaat_t <- kateUlkomaat_t - kateUlkomaat_tMin1
dKateUlkomaat_t <- dKateUlkomaat_t/kateUlkomaat_tMin1
dKateUlkomaat_t <- dKateUlkomaat_t*100
dKateUlkomaat_t[dKateUlkomaat_t == 'NaN'] <- 0
dKateUlkomaat_t[dKateUlkomaat_t == 'Inf'] <- 0
cbind(dKateUlkomaat_t, vuosi_t)
dKateUlkomaat2005_2016 <- subset(dKateUlkomaat_t, vuosi_t >2004)
mean(dKateUlkomaat2005_2016)
# Piirettään muutoskuvaaja
plot(vuosi_t ,dKateKotimaa_t, ylim = c(-12, 30), xlim = c(2005,2016), type = "o", col = "blue", xlab = "Vuosi", ylab = "Muutos (%)")
lines(vuosi_t, dKateUlkomaat_t, type = "o", col  = "red")
#########################################################
pelikateKok_t <- kateUlkomaat_t+kateKotimaa_t
pelikateKok_tMin1 <- kateUlkomaat_tMin1+kateKotimaa_tMin1
pelikateMuutos_t <- pelikateKok_t/pelikateKok_tMin1
(pelikateMuutos_t-1)*100
ulkomaidenOsuusTuloista <- kateUlkomaat_t/pelikateKok_t
#########################################################
summary(data_R_kaikki_1990_2016)
250,28*9
250.28*9
3*2200
?head
## In this example, the data is in a matrix called
## data.matrix
## columns are individual samples (i.e. cells)
## rows are measurements taken for all the samples (i.e. genes)
## Just for the sake of the example, here's some made up data...
data.matrix <- matrix(nrow=100, ncol=10)
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""),
paste("ko", 1:5, sep=""))
rownames(data.matrix) <- paste("gene", 1:100, sep="")
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i,] <- c(wt.values, ko.values)
}
head(data.matrix)
dim(data.matrix)
?prcomp
pca <- prcomp(t(data.matrix), scale=TRUE)
## plot pc1 and pc2
plot(pca$x[,1], pca$x[,2])
## make a scree plot
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
## now make a fancy looking plot that shows the PCs and the variation:
library(ggplot2)
install.packages("ggplot2")
pca.data <- data.frame(Sample=rownames(pca$x),
X=pca$x[,1],
Y=pca$x[,2])
pca.data
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("My PCA Graph")
library("ggplot2", lib.loc="~/R/win-library/3.4")
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("My PCA Graph")
library("ggplot2", lib.loc="~/R/win-library/3.4")
install.packages("lazyeval")
install.packages("lazyeval")
library("ggplot2", lib.loc="~/R/win-library/3.4")
## In this example, the data is in a matrix called
## data.matrix
## columns are individual samples (i.e. cells)
## rows are measurements taken for all the samples (i.e. genes)
## Just for the sake of the example, here's some made up data...
data.matrix <- matrix(nrow=100, ncol=10)
colnames(data.matrix) <- c(
paste("wt", 1:5, sep=""),
paste("ko", 1:5, sep=""))
rownames(data.matrix) <- paste("gene", 1:100, sep="")
for (i in 1:100) {
wt.values <- rpois(5, lambda=sample(x=10:1000, size=1))
ko.values <- rpois(5, lambda=sample(x=10:1000, size=1))
data.matrix[i,] <- c(wt.values, ko.values)
}
head(data.matrix)
dim(data.matrix)
pca <- prcomp(t(data.matrix), scale=TRUE)
## plot pc1 and pc2
plot(pca$x[,1], pca$x[,2])
## make a scree plot
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
## now make a fancy looking plot that shows the PCs and the variation:
library(ggplot2)
install.packages("rlang")
install.packages("rlang")
install.packages("tidyverse", dependencies = TRUE)
## now make a fancy looking plot that shows the PCs and the variation:
library(ggplot2)
pca.data <- data.frame(Sample=rownames(pca$x),
X=pca$x[,1],
Y=pca$x[,2])
pca.data
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("My PCA Graph")
## get the name of the top 10 measurements (genes) that contribute
## most to pc1.
loading_scores <- pca$rotation[,1]
gene_scores <- abs(loading_scores) ## get the magnitudes
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes ## show the names of the top 10 genes
pca$rotation[top_10_genes,1] ## show the scores (and +/- sign)
svd.stuff <- svd(scale(t(data.matrix), center=TRUE))
## calculate the PCs
svd.data <- data.frame(Sample=colnames(data.matrix),
X=(svd.stuff$u[,1] * svd.stuff$d[1]),
Y=(svd.stuff$u[,2] * svd.stuff$d[2]))
svd.data
## alternatively, we could compute the PCs with the eigen vectors and the
## original data
svd.pcs <- t(t(svd.stuff$v) %*% t(scale(t(data.matrix), center=TRUE)))
svd.pcs[,1:2] ## the first to principal components
svd.df <- ncol(data.matrix) - 1
svd.var <- svd.stuff$d^2 / svd.df
svd.var.per <- round(svd.var/sum(svd.var)*100, 1)
ggplot(data=svd.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", svd.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", svd.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("svd(scale(t(data.matrix), center=TRUE)")
############################################
##
## Now let's do the same thing with eigen()
##
## eigen() returns two things...
## vectors = eigen vectors (vectors of loading scores)
##           NOTE: pcs = sum(loading scores * values for sample)
## values = eigen values
############################################
cov.mat <- cov(scale(t(data.matrix), center=TRUE))
dim(cov.mat)
## since the covariance matrix is symmetric, we can tell eigen() to just
## work on the lower triangle with "symmetric=TRUE"
eigen.stuff <- eigen(cov.mat, symmetric=TRUE)
dim(eigen.stuff$vectors)
head(eigen.stuff$vectors[,1:2])
eigen.pcs <- t(t(eigen.stuff$vectors) %*% t(scale(t(data.matrix), center=TRUE)))
eigen.pcs[,1:2]
eigen.data <- data.frame(Sample=rownames(eigen.pcs),
X=(-1 * eigen.pcs[,1]), ## eigen() flips the X-axis in this case, so we flip it back
Y=eigen.pcs[,2]) ## X axis will be PC1, Y axis will be PC2
eigen.data
eigen.var.per <- round(eigen.stuff$values/sum(eigen.stuff$values)*100, 1)
ggplot(data=eigen.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", eigen.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", eigen.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("eigen on cov(t(data.matrix))")
?martix
?matrix()
X <- matrix(c(1,3,1,2,1,0), nrow = 3)
X
X <- matrix(c(1,1,1,3,2,0), nrow = 3)
X
beta_real <- matrix(c(-1,2), nrow = 2)
beta_real
epsilon <- (c(-0.1, 0.4, -0.3), nrow = 3)
epsilon <- matrix(c(-0.1, 0.4, -0.3), nrow = 3)
epsi
epsilon
epsilon_real <- matrix(c(-0.1, 0.4, -0.3), nrow = 3)
epsilon_real
X <- matrix(c(1,1,1,3,2,0), nrow = 3)
X
beta_real <- matrix(c(-1,2), nrow = 2)
beta_real
epsilon_real <- matrix(c(-0.1, 0.4, -0.3), nrow = 3)
epsilon_real
epsilon_act <- matrix(c(-0.1, 0.4, -0.3), nrow = 3)
epsilon_act   #Simulated from normal disripution, mean = 0 and standard deviation = 0.1
y_act <- X*beta_real + epsilon_act
y_act <- X*beta_real
X
beta_real
y_act <- X%*%beta_real + epsilon_act
y_act
lm(y_act~X)
projection_x <- X
count_prjojection(X, projection_x)
count_prjojection <-  function(X, projection_x) {
projection_x<-t(X)%*%X
projection_x<-solve(projection_x)
projection_x<-X%*%projection_x
projection_x<-projection_x%*%t(X)
}
projection_x <- X
count_prjojection(X, projection_x)
projection_x
projection_x <- X
projection_x
count_prjojection(X, projection_x)
projection_x
count_prjojection <-  function(X) {
projection_x<-t(X)%*%X
projection_x<-solve(projection_x)
projection_x<-X%*%projection_x
projection_x<-projection_x%*%t(X)
return(projection_x)
}
count_prjojection(X)
projection_x<-count_prjojection(X)
projection_x
y_hat <- projection_x%*%y_act
y_hat
I <- matrix(c(1,0,0,0,1,0,0,0,1), nrow = 3)
I
residual <-(I-projection_x)%*%y_act
residual
X
X1 <- matrix(c(1,1,1), nrow = 3)
X1
X2<- matrix(c(3,2,0), nrow = 3)
X2
projection_x1 <- count_prjojection(X1)
I
Mx1 <- I - projection_x1
Mx1%*%X2
Mx1%*%X1
projection_x
eigen_projection_x <- eigen(projection_x)
eigen_projection_x$values
eigen_projection_x1 <- eigen(projection_x1)
eigen_projection_x1$values
library(readr)
talon_kulmat <- read_csv("Random kansio/talon_kulmat.txt")
View(talon_kulmat)
coordinates_matrix <- talon_kulmat
schoelace_fun <- function(coordinates) {
areas <- rep.int(0, nrow(coordinates))
for(i in 1:c(nrow(coordinates)-1)) {
det <- coordinates[i,1]*coordinates[i+1,2]-coordinates[i+1,1]*coordinates[i,2]
area <- 0.5*det
areas[i] <- area
}
print(sum(areas))
}
area_house_px <- schoelace_fun(coordinates_matrix)
coordinates_matrix <- as.matrx(talon_kulmat)
coordinates_matrix <- matrix(talon_kulmat)
coordinates_matrix
coordinates_matrix <- as.matrix(talon_kulmat)
coordinates_matrix
schoelace_fun <- function(coordinates) {
areas <- rep.int(0, nrow(coordinates))
for(i in 1:c(nrow(coordinates)-1)) {
det <- coordinates[i,1]*coordinates[i+1,2]-coordinates[i+1,1]*coordinates[i,2]
area <- 0.5*det
areas[i] <- area
}
print(sum(areas))
}
area_house_px <- schoelace_fun(coordinates_matrix)
area_house_real <- 78
scaler <- sqrt(area_house_real/-area_house_px)
lengths_fun <- function(coordinates){
lengths <- rep.int(0,nrow(coordinates)-1)
for(j in 1:c(nrow(coordinates)-1)) {
dif_y <- coordinates[j,2] - coordinates[j+1,2]
dif_x <- coordinates[j,1] - coordinates[j+1,1]
length <- sqrt((dif_y^2)+(dif_x^2))
lengths[j] <- length
}
print(scaler*lengths)
}
lengths_fun(coordinates_matrix)
View(coordinates_matrix, "Seinien pituudet")
length_walls <- lengths_fun(coordinates_matrix)
View(length_walls, "Seinien pituudet")
?sink()
length_walls <- capture.output(lengths_fun(coordinates_matrix))
cat("Seinien pituudet", length_walls, file = "C:/Users/juhol/OneDrive/Documents/seinien_pituudet.csv", sep = "n")
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
str(lrn14)
dim(lrn14)
library(dplyr)
# questions related to deep, surface and strategic learning
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
# select the columns related to deep learning and create column 'deep' by averaging
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
# select the columns related to surface learning and create column 'surf' by averaging
surface_columns <- select(lrn14, one_of(surface_questions))
lrn14$surf <- rowMeans(surface_columns)
# select the columns related to strategic learning and create column 'stra' by averaging
strategic_columns <- select(lrn14, one_of(strategic_questions))
lrn14$stra <- rowMeans(strategic_columns)
# choose a handful of columns to keep
keep_columns <- c("gender","Age","attitude", "deep", "stra", "surf", "Points")
# select the 'keep_columns' to create a new dataset
learning2014 <- select(lrn14, one_of(keep_columns))
# see the stucture of the new dataset
str(learning2014)
colnames(learning2014)
colnames(learning2014)[2] <- "age"
# change the name of "Points" to "points"
colnames(learning2014)[7] <- "points"
# print out the new column names of the data
colnames(learning2014)
# change the name of "Points" to "points"
colnames(learning2014)[7] <- "points"
# print out the new column names of the data
colnames(learning2014)
# change the name of "Points" to "points"
colnames(learning2014)[6] <- "points"
# print out the new column names of the data
colnames(learning2014)
library(dplyr)
# Juho Lähteenmaa
# 09/11/2018
# IODS, week 2.
# Data wrangling
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
# Structure of data
str(lrn14)
# Dimensions of data
dim(lrn14)
# 60 variables and 183 observations
# questions related to deep, surface and strategic learning
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
# select the columns related to deep learning and create column 'deep' by averaging
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
# select the columns related to surface learning and create column 'surf' by averaging
surface_columns <- select(lrn14, one_of(surface_questions))
lrn14$surf <- rowMeans(surface_columns)
# select the columns related to strategic learning and create column 'stra' by averaging
strategic_columns <- select(lrn14, one_of(strategic_questions))
lrn14$stra <- rowMeans(strategic_columns)
# choose a handful of columns to keep
keep_columns <- c("gender","Age","attitude", "deep", "stra", "surf", "Points")
# select the 'keep_columns' to create a new dataset
learning2014 <- select(lrn14, one_of(keep_columns))
# see the stucture of the new dataset
str(learning2014)
# Juho Lähteenmaa
# 09/11/2018
# IODS, week 2.
# Data wrangling
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
# Structure of data
str(lrn14)
# Dimensions of data
dim(lrn14)
# 60 variables and 183 observations
# questions related to deep, surface and strategic learning
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
# select the columns related to deep learning and create column 'deep' by averaging
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
# select the columns related to surface learning and create column 'surf' by averaging
surface_columns <- select(lrn14, one_of(surface_questions))
lrn14$surf <- rowMeans(surface_columns)
# select the columns related to strategic learning and create column 'stra' by averaging
strategic_columns <- select(lrn14, one_of(strategic_questions))
lrn14$stra <- rowMeans(strategic_columns)
# choose a handful of columns to keep
keep_columns <- c("gender","Age","attitude", "deep", "stra", "surf", "Points")
# select the 'keep_columns' to create a new dataset
learning2014 <- select(lrn14, one_of(keep_columns))
# see the stucture of the new dataset
str(learning2014
str(learning2014)
str(learning2014)
keep_columns
# select the 'keep_columns' to create a new dataset
learning2014 <- select(lrn14, one_of(keep_columns))
# Juho Lähteenmaa
# 09/11/2018
# IODS, week 2.
# Data wrangling
lrn14 <- read.table("http://www.helsinki.fi/~kvehkala/JYTmooc/JYTOPKYS3-data.txt", sep="\t", header=TRUE)
# Structure of data
str(lrn14)
# Dimensions of data
dim(lrn14)
# 60 variables and 183 observations
# print the "Attitude" column vector of the lrn14 data
lrn14$Attitude
# divide each number in the column vector
lrn14$Attitude / 10
# create column 'attitude' by scaling the column "Attitude"
lrn14$attitude <- lrn14$Attitude/10
# questions related to deep, surface and strategic learning
deep_questions <- c("D03", "D11", "D19", "D27", "D07", "D14", "D22", "D30","D06",  "D15", "D23", "D31")
surface_questions <- c("SU02","SU10","SU18","SU26", "SU05","SU13","SU21","SU29","SU08","SU16","SU24","SU32")
strategic_questions <- c("ST01","ST09","ST17","ST25","ST04","ST12","ST20","ST28")
# select the columns related to deep learning and create column 'deep' by averaging
deep_columns <- select(lrn14, one_of(deep_questions))
lrn14$deep <- rowMeans(deep_columns)
# select the columns related to surface learning and create column 'surf' by averaging
surface_columns <- select(lrn14, one_of(surface_questions))
lrn14$surf <- rowMeans(surface_columns)
# select the columns related to strategic learning and create column 'stra' by averaging
strategic_columns <- select(lrn14, one_of(strategic_questions))
lrn14$stra <- rowMeans(strategic_columns)
# choose a handful of columns to keep
keep_columns <- c("gender","Age","attitude", "deep", "stra", "surf", "Points")
# select the 'keep_columns' to create a new dataset
learning2014 <- select(lrn14, one_of(keep_columns))
# see the stucture of the new dataset
str(learning2014)
# print out the column names of the data
colnames(learning2014)
# change the name of the second column
colnames(learning2014)[2] <- "age"
# change the name of "Points" to "points"
colnames(learning2014)[6] <- "points"
# print out the new column names of the data
colnames(learning2014)
# change the name of "Points" to "points"
colnames(learning2014)[6] <- "surf"
# print out the new column names of the data
colnames(learning2014)
# change the name of "Points" to "points"
colnames(learning2014)[7] <- "points"
# print out the new column names of the data
colnames(learning2014)
getwd()
setwd(/c/Users/juhol/OneDrive/Documents/GitHub/IODS-project)
setwd(c/Users/juhol/OneDrive/Documents/GitHub/IODS-project)
getwd()
setwd(C:/Users/juhol/OneDrive/Documents/GitHub/IODS-project)
setwd("C:/Users/juhol/OneDrive/Documents/GitHub/IODS-project")
getwd()
dir()
setwd("C:/Users/juhol/OneDrive/Documents/GitHub/IODS-project/data")
getwd()
?write.csv()
write.csv(learning2014, file = "learning2014.csv")
?read.csv()
